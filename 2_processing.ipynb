{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(416809, 2)\n",
      "emotions\n",
      "joy         0.338445\n",
      "sadness     0.290749\n",
      "anger       0.137514\n",
      "fear        0.114470\n",
      "love        0.082901\n",
      "surprise    0.035921\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(\"data/data_original.pkl\")\n",
    "\n",
    "# undersample the data in 10%, distributing the classes proportionally in a stratified way and with a fixed seed\n",
    "# df = df.groupby(\"emotions\", group_keys=False).apply(lambda x: x.sample(frac=0.1, random_state=42))\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "# print proportion of each class\n",
    "print(df[\"emotions\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some basic cleaning using regex\n",
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "# remove href link\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'href', '', x)) \n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'http', '', x))\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'www', '', x))\n",
    "\n",
    "# remove punctuation, numbers, and extra spaces\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\n', ' ', x))\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "df['text'] = df['text'].str.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with stopwords and negations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the dataset is very badly written, we need to consider words written with apostrophes, spaces or just connected to other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'once', 'aren t', 'has', \"haven't\", 'again', 'which', 'yours', 'does', \"couldn't\", 'their', 'any', 'only', 'shan t', 'werent', 'they', 'shant', 'whom', 'me', 'a', 'down', 'didnt', 'too', \"you're\", 'don', 'wouldnt', 'he', 'd', 'against', 'isnt', 'to', 'it s', 'this', 'youve', 'shouldnt', 'won', 'didn t', 'above', \"wouldn't\", 'ourselves', \"mustn't\", 'mightn', 'she', 'my', 'into', \"it's\", 'who', \"you'd\", \"needn't\", 'same', 'mightn t', 'further', 'youre', 'haven t', 'being', 'youll', 'shan', 'doing', 'some', 've', \"shouldn't\", 'should ve', 'we', 'as', 'couldn', \"mightn't\", 'under', 'o', 'mightnt', 'on', 'doesn', 'youd', 'yourself', 'before', \"wasn't\", 'dont', 'why', 'them', 'didn', 'when', 'what', 'there', 'doesnt', 'wouldn', 'won t', 'have', 'hadn t', 'out', 'through', 'yourselves', 'below', 'just', 'mustn', 'weren t', 'are', 'during', 'than', 'had', 'not', 'then', \"doesn't\", \"didn't\", 'or', 'you', 'theirs', \"won't\", 'where', 'such', 'weren', 'that ll', 'hasn', 'for', 'that', 'wouldn t', 'herself', \"you'll\", 'it', \"hadn't\", 'up', 'and', 'while', 'hadnt', 'off', 'few', 'needn t', 'him', 'itself', \"you've\", 'hers', 'wasnt', 'himself', 'an', 'hadn', 'all', 'in', 'wasn t', 'needn', 'neednt', 'm', 'myself', 'until', 'should', 'those', 'very', 'ain', 'own', 'ours', 'about', 'his', 'now', 'wasn', 'arent', 'shouldn t', \"don't\", 'i', 'y', 'shouldn', 'hasnt', 'couldnt', \"shan't\", \"isn't\", 'no', 'been', \"that'll\", 'more', 'wont', 'our', 'your', 'couldn t', 'thatll', 'after', 's', 'll', 'you d', 'am', \"weren't\", 'she s', 'so', 'is', 'themselves', \"should've\", 'her', 'these', 'other', 'shouldve', \"she's\", 'between', 'doesn t', 'having', 'how', 'nor', 'will', 're', 'don t', 'mustn t', 'over', 'most', 'can', \"aren't\", 'both', 'ma', 'with', 'havent', 'were', \"hasn't\", 'shes', 'hasn t', 'at', 'here', 'but', 'you re', 'each', 't', 'be', 'isn', 'mustnt', 'isn t', 'the', 'because', 'by', 'haven', 'you ve', 'was', 'do', 'of', 'if', 'aren', 'you ll', 'its', 'did', 'from'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# if there is a stopword with an apostrophe, add the word without the apostrophe and a space instead of the apostrophe, and the word without the apostrophe\n",
    "more_stop_words = set()\n",
    "for word in stop_words:\n",
    "    if \"'\" in word:\n",
    "        more_stop_words.add(word.replace(\"'\", \" \"))\n",
    "        more_stop_words.add(word.replace(\"'\", \"\"))\n",
    "stop_words = stop_words.union(more_stop_words)\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of negation words, we need to consider the words that are connected to them, so we can preserve the meaning of the sentence by preappending the negation word with a \"not_\" prefix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aren t', \"hadn't\", \"haven't\", 'hadnt', \"couldn't\", 'needn t', 'shan t', 'werent', 'shant', 'wasnt', 'hadn', 'wasn t', 'didnt', 'needn', 'neednt', 'wouldnt', 'ain', 'isnt', 'shouldnt', 'wasn', 'arent', 'won', 'shouldn t', 'didn t', \"wouldn't\", 'shouldn', 'hasnt', 'couldnt', \"mustn't\", 'mightn', \"shan't\", \"isn't\", 'no', 'wont', \"needn't\", 'mightn t', 'couldn t', 'haven t', \"weren't\", 'shan', \"shouldn't\", 'doesn t', 'nor', 'cant', 'couldn', \"mightn't\", 'mightnt', 'mustn t', 'doesn', \"aren't\", 'ma', \"wasn't\", 'havent', 'didn', \"hasn't\", 'hasn t', 'hadn t', 'doesnt', 'wouldn', 'won t', 'isn', 'weren t', 'mustn', \"can't\", 'mustnt', 'isn t', 'not', \"doesn't\", \"didn't\", 'haven', 'can t', 'aren', 'weren', \"won't\", 'hasn', 'wouldn t'}\n"
     ]
    }
   ],
   "source": [
    "# remove negative words from stop_words list\n",
    "\n",
    "neg_stop_words = {'no', 'nor', 'not', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", \"can't\", \"cant\"}\n",
    "\n",
    "more_neg_words = set()\n",
    "# for each negative word with an apostrophe, we will add the word without the apostrophe and a space instead of the apostrophe\n",
    "for word in neg_stop_words:\n",
    "    if \"'\" in word:\n",
    "        more_neg_words.add(word.replace(\"'\", \" \"))\n",
    "        more_neg_words.add(word.replace(\"'\", \"\"))\n",
    "\n",
    "neg_stop_words = neg_stop_words.union(more_neg_words)\n",
    "\n",
    "print(neg_stop_words)\n",
    "\n",
    "# remove from stop_words the negative words\n",
    "stop_words = stop_words - neg_stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "# if a word is in neg_stop_words, we will add a prefix \"not_\" to the word that follows it and remove the negative word\n",
    "def add_not_prefix(text):\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        if words[i] in neg_stop_words:\n",
    "            try:\n",
    "                new_words.append('not_' + words[i+1])\n",
    "            except:\n",
    "                new_words.append('not') # if the negative word is the last word, just add \"not\"\n",
    "            i += 2\n",
    "        else:\n",
    "            new_words.append(words[i])\n",
    "            i += 1\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "df['text'] = df['text'].apply(add_not_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with small words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focusing now on small words, they are not very informative and are in many instances just noise. We can remove them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of small words is 0.13%\n",
      "The 30 most common small words are:\n",
      "u    798\n",
      "b    608\n",
      "n    520\n",
      "c    432\n",
      "e    404\n",
      "p    384\n",
      "x    339\n",
      "w    281\n",
      "k    246\n",
      "r    212\n",
      "f    203\n",
      "j    165\n",
      "g    153\n",
      "l    138\n",
      "h    126\n",
      "v     85\n",
      "z     55\n",
      "q     38\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display percentage of words in the dataset that are less than 3 characters \n",
    "\n",
    "words = df['text'].str.split(expand=True).stack()\n",
    "\n",
    "# Get set of unique small words\n",
    "small_words = words[words.str.len() < 2]\n",
    "\n",
    "print(f\"The percentage of small words is {round(small_words[small_words.str.len() < 2].count() / words.count() * 100, 2)}%\") \n",
    "\n",
    "# show the 30 most common small words\n",
    "print(f\"The 30 most common small words are:\\n{small_words.value_counts().head(30)}\") \n",
    "\n",
    "# remove small words from the dataset and keep the ones in keep_small_words\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if len(word) >= 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with bad written words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A thing that we noticed in the dataset is that there are a lot of words that are not written correctly. We can use a spell checker to correct them. But first, one thing that we can do is fix words that are written with more than one letter repeated, like \"soooooo\" or \"amaaaaaazinggggg\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all consecutive same letters\n",
    "def replace_repetitive(text):\n",
    "    pattern = r'(\\w)\\1{2,}'\n",
    "    replaced_text = re.sub(pattern, r'\\1\\1', text)\n",
    "    return replaced_text\n",
    "\n",
    "df['text'] = df['text'].apply(replace_repetitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming and lemmatization\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "def stem(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(stem)\n",
    "# df[\"text\"] = df[\"text\"].apply(lemmatize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27383              feel aw job get posit succeed not_happen\n",
      "110083                                      im alon feel aw\n",
      "140764    ive probabl mention realli feel proud actual k...\n",
      "100071                              feel littl low day back\n",
      "2837          beleiv much sensit peopl feel tend compassion\n",
      "18231     find frustrat christian feel constantli talk l...\n",
      "10714             one peopl feel like go gym worthwhil hour\n",
      "35177                      feel especi pleas long time come\n",
      "122177    struggl aw feel say sweet thing not_deserv sis...\n",
      "26723                              feel enrag helpless time\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Show the first 10 rows\n",
    "print(df[\"text\"].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>416809</td>\n",
       "      <td>416809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>379880</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>feel accept</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>65</td>\n",
       "      <td>141067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               text emotions\n",
       "count        416809   416809\n",
       "unique       379880        6\n",
       "top     feel accept      joy\n",
       "freq             65   141067"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df to pickle\n",
    "df.to_pickle(\"data/data_processed.pkl\") # NOTE: ONLY SAVING 10% OF THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# apply spacy and save to pickle\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Does the following:\n",
    "# 1. Remove non alpha characters\n",
    "# 2. Convert to lower-case\n",
    "# 3. Tokenize the sentence\n",
    "# 4. POS Tagging\n",
    "# 5. Dependency Parsing\n",
    "# 6. Named Entity Recognition\n",
    "# 7. Lemmatization\n",
    "# 8. Remove stop words\n",
    "# 9. Sentence Boundary Detection\n",
    "def spacy_pre_process(sentence):\n",
    "    global nlp\n",
    "    return nlp(sentence)\n",
    "\n",
    "df['text'] = df['text'].apply(spacy_pre_process) # TAKES A LOT OF TIME\n",
    "\n",
    "df.to_pickle(\"data/data_spacy_processed.pkl\") # NOTE: ONLY SAVING 10% OF THE DATA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
