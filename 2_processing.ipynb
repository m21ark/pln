{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41681, 2)\n",
      "emotions\n",
      "joy         0.338452\n",
      "sadness     0.290756\n",
      "anger       0.137521\n",
      "fear        0.114465\n",
      "love        0.082891\n",
      "surprise    0.035916\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(\"data/data_original.pkl\")\n",
    "\n",
    "# undersample the data in 10%, distributing the classes proportionally in a stratified way and with a fixed seed\n",
    "df = df.groupby(\"emotions\", group_keys=False).apply(lambda x: x.sample(frac=0.1, random_state=42))\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "# print proportion of each class\n",
    "print(df[\"emotions\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some basic cleaning using regex\n",
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "# remove href link\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'href', '', x)) \n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'http', '', x))\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'www', '', x))\n",
    "\n",
    "# remove punctuation, numbers, and extra spaces\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\n', ' ', x))\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "df['text'] = df['text'].str.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with stopwords and negations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the dataset is very badly written, we need to consider words written with apostrophes, spaces or just connected to other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'be', 'any', 'your', \"haven't\", \"isn't\", 'it', \"aren't\", 'and', 'youd', 'or', 'under', 'll', 'youll', 'should', 'mustn t', \"don't\", 'some', 'what', 'if', 'each', 'wouldn', 'neednt', 'i', 'youre', 'of', 'do', 'don', 'through', 'weren t', 'those', 'wouldnt', 'dont', \"mightn't\", 'don t', 'where', 'at', 'thatll', 'just', 'shes', 'its', \"hasn't\", 'wasn', 'couldn', 'having', 'he', 'ma', 'you re', 'same', 'didnt', 'than', 'our', 'for', 'about', 'havent', 'haven t', \"shan't\", 'wouldn t', 'with', 'themselves', 'we', 'shant', 'doesnt', 'between', 'such', \"won't\", 'here', 'am', \"didn't\", 'yours', 'as', 'over', 'weren', 'shan t', 'these', 'himself', 'it s', 'd', 'his', 'mustn', 'isn t', 'myself', 'other', 'me', 'on', 'did', \"should've\", 'o', 'm', 'this', 'shouldve', 'until', 'mightnt', 'during', 'below', 're', \"doesn't\", \"needn't\", 'won', 'are', 'him', 'before', 'both', 'herself', 'their', 'was', 'then', \"weren't\", 'hasnt', 'ours', 'shan', \"you're\", 'because', 's', 'hasn t', 'y', 'itself', 'that ll', 'off', 'my', 'not', 'above', 'hers', 'isnt', 'doesn t', 'how', 'you ve', \"shouldn't\", 'had', 'mustnt', 'should ve', 'all', 'couldn t', 'wasn t', 'being', 'which', 'didn', 'you d', \"it's\", 'theirs', 'to', 'yourselves', 'them', \"hadn't\", 'when', 'haven', 'most', 'a', 't', 'shouldnt', 'you ll', 'she', 'couldnt', 'does', 'isn', 'mightn', 'hasn', 'can', 'you', 'few', 'too', 'wont', 'will', 'after', 'aren', 'out', 'in', 'were', 'won t', 'ain', 'wasnt', 'yourself', 'now', 'doing', 'whom', 'shouldn', 'doesn', 'more', \"couldn't\", 'an', 'into', \"wasn't\", 'aren t', 'needn t', 'by', 'from', \"you've\", 'arent', 'has', 'down', 'didn t', 'who', 'no', 'again', 'werent', 'the', 'up', \"you'll\", 'they', 'needn', 'why', 'but', 'ourselves', 'mightn t', 'while', \"you'd\", 'nor', 'further', \"that'll\", \"wouldn't\", 'only', 'her', 'that', 'youve', 'been', 'there', 'against', 've', 'is', 'very', 'own', 'she s', 'so', 'hadn', 'shouldn t', 'hadnt', 'hadn t', 'once', \"she's\", 'have', \"mustn't\"}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# if there is a stopword with an apostrophe, add the word without the apostrophe and a space instead of the apostrophe, and the word without the apostrophe\n",
    "more_stop_words = set()\n",
    "for word in stop_words:\n",
    "    if \"'\" in word:\n",
    "        more_stop_words.add(word.replace(\"'\", \" \"))\n",
    "        more_stop_words.add(word.replace(\"'\", \"\"))\n",
    "stop_words = stop_words.union(more_stop_words)\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of negation words, we need to consider the words that are connected to them, so we can preserve the meaning of the sentence by preappending the negation word with a \"not_\" prefix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'isnt', \"haven't\", \"isn't\", \"aren't\", 'doesn t', \"shouldn't\", 'mustn t', 'mustnt', 'couldn t', 'wasn t', 'wouldn', 'neednt', 'wouldnt', 'weren t', 'didn', \"mightn't\", \"hadn't\", 'haven', 'wasn', \"hasn't\", 'couldn', 'shouldnt', 'ma', 'couldnt', 'isn', 'mightn', 'hasn', 'didnt', 'wont', 'aren', 'havent', 'haven t', \"shan't\", 'won t', 'wouldn t', 'ain', 'wasnt', 'shant', 'shouldn', 'doesn', 'doesnt', \"couldn't\", \"won't\", \"wasn't\", 'cant', \"didn't\", 'aren t', 'weren', 'needn t', 'arent', 'shan t', 'didn t', 'isn t', 'no', 'mustn', 'werent', 'needn', 'mightn t', 'mightnt', 'nor', \"doesn't\", \"needn't\", 'won', \"wouldn't\", \"weren't\", 'hasnt', 'shan', 'hasn t', 'hadn', \"can't\", 'shouldn t', 'hadnt', 'hadn t', 'can t', 'not', \"mustn't\"}\n"
     ]
    }
   ],
   "source": [
    "# remove negative words from stop_words list\n",
    "\n",
    "neg_stop_words = {'no', 'nor', 'not', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", \"can't\", \"cant\"}\n",
    "\n",
    "more_neg_words = set()\n",
    "# for each negative word with an apostrophe, we will add the word without the apostrophe and a space instead of the apostrophe\n",
    "for word in neg_stop_words:\n",
    "    if \"'\" in word:\n",
    "        more_neg_words.add(word.replace(\"'\", \" \"))\n",
    "        more_neg_words.add(word.replace(\"'\", \"\"))\n",
    "\n",
    "neg_stop_words = neg_stop_words.union(more_neg_words)\n",
    "\n",
    "print(neg_stop_words)\n",
    "\n",
    "# remove from stop_words the negative words\n",
    "stop_words = stop_words - neg_stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "# if a word is in neg_stop_words, we will add a prefix \"not_\" to the word that follows it and remove the negative word\n",
    "def add_not_prefix(text):\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        if words[i] in neg_stop_words:\n",
    "            try:\n",
    "                new_words.append('not_' + words[i+1])\n",
    "            except:\n",
    "                new_words.append('not') # if the negative word is the last word, just add \"not\"\n",
    "            i += 2\n",
    "        else:\n",
    "            new_words.append(words[i])\n",
    "            i += 1\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "df['text'] = df['text'].apply(add_not_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with small words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focusing now on small words, they are not very informative and are in many instances just noise. We can remove them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of small words is 0.13%\n",
      "The 30 most common small words are:\n",
      "u    72\n",
      "n    56\n",
      "b    54\n",
      "c    48\n",
      "e    45\n",
      "w    42\n",
      "p    31\n",
      "x    25\n",
      "f    21\n",
      "k    19\n",
      "r    19\n",
      "g    18\n",
      "j    14\n",
      "l    14\n",
      "h    12\n",
      "v     9\n",
      "q     8\n",
      "z     3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display percentage of words in the dataset that are less than 3 characters \n",
    "\n",
    "words = df['text'].str.split(expand=True).stack()\n",
    "\n",
    "# Get set of unique small words\n",
    "small_words = words[words.str.len() < 2]\n",
    "\n",
    "print(f\"The percentage of small words is {round(small_words[small_words.str.len() < 2].count() / words.count() * 100, 2)}%\") \n",
    "\n",
    "# show the 30 most common small words\n",
    "print(f\"The 30 most common small words are:\\n{small_words.value_counts().head(30)}\") \n",
    "\n",
    "# remove small words from the dataset and keep the ones in keep_small_words\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if len(word) >= 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with bad written words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A thing that we noticed in the dataset is that there are a lot of words that are not written correctly. We can use a spell checker to correct them. But first, one thing that we can do is fix words that are written with more than one letter repeated, like \"soooooo\" or \"amaaaaaazinggggg\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all consecutive same letters\n",
    "def replace_repetitive(text):\n",
    "    pattern = r'(\\w)\\1{2,}'\n",
    "    replaced_text = re.sub(pattern, r'\\1\\1', text)\n",
    "    return replaced_text\n",
    "\n",
    "df['text'] = df['text'].apply(replace_repetitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming and lemmatization\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "def stem(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(stem)\n",
    "# df[\"text\"] = df[\"text\"].apply(lemmatize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19132                           feel irrit kinda hate feel\n",
      "51533    id rather home feel violent lone im not_tri so...\n",
      "44351                suggest wait discuss feel less resent\n",
      "51299                                wrong feel royal piss\n",
      "55778    im tierd talk like there hope hell care unders...\n",
      "17018    feel frustrat honest like not_get money worth ...\n",
      "45995    tri make chang feel urg happen particularli di...\n",
      "45862                    truli good feel fight ever bother\n",
      "48826                                          feel jealou\n",
      "14602                       feel angri think like elsewher\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Show the first 10 rows\n",
    "print(df[\"text\"].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df to pickle\n",
    "df.to_pickle(\"data/data_processed.pkl\") # NOTE: ONLY SAVING 10% OF THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m/.local/lib/python3.10/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# apply spacy and save to pickle\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Does the following:\n",
    "# 1. Remove non alpha characters\n",
    "# 2. Convert to lower-case\n",
    "# 3. Tokenize the sentence\n",
    "# 4. POS Tagging\n",
    "# 5. Dependency Parsing\n",
    "# 6. Named Entity Recognition\n",
    "# 7. Lemmatization\n",
    "# 8. Remove stop words\n",
    "# 9. Sentence Boundary Detection\n",
    "def spacy_pre_process(sentence):\n",
    "    global nlp\n",
    "    return nlp(sentence)\n",
    "\n",
    "df['text'] = df['text'].apply(spacy_pre_process) # TAKES A LOT OF TIME\n",
    "\n",
    "df.to_pickle(\"data/data_spacy_processed.pkl\") # NOTE: ONLY SAVING 10% OF THE DATA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
