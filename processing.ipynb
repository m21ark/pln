{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41681, 2)\n",
      "emotions\n",
      "joy         0.338452\n",
      "sadness     0.290756\n",
      "anger       0.137521\n",
      "fear        0.114465\n",
      "love        0.082891\n",
      "surprise    0.035916\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(\"models/data_original.pkl\")\n",
    "\n",
    "# undersample the data in 10%, distributing the classes proportionally in a stratified way and with a fixed seed\n",
    "df = df.groupby(\"emotions\", group_keys=False).apply(lambda x: x.sample(frac=0.1, random_state=42))\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "# print proportion of each class\n",
    "print(df[\"emotions\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some basic cleaning using regex\n",
    "\n",
    "df['text'] = df['text'].str.lower()\n",
    "\n",
    "# remove href link\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'href', '', x)) \n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'http', '', x))\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'www', '', x))\n",
    "\n",
    "# remove punctuation, numbers, and extra spaces\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\n', ' ', x))\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "df['text'] = df['text'].str.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with stopwords and negations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the dataset is very badly written, we need to consider words written with apostrophes, spaces or just connected to other words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'down', 'he', \"you're\", 'or', 'd', 'won', \"doesn't\", 'myself', 'has', 'each', 'more', 've', 'mightn', 'having', 'are', 'weren', 'won t', 'can', 'both', 'once', \"it's\", 'ours', 'few', 'into', 'wasn', 'than', 'yourselves', 'because', 'just', 'you ll', 'have', 'doesn', 'don', 'hadn t', 'there', \"shan't\", 'yourself', 'where', 'hasn', 'below', 'some', 'neednt', 'all', 'they', 'after', \"mustn't\", 'youve', 'hadnt', 'whom', 'doing', 'the', 'we', 'themselves', 'were', 'out', 'youre', 'it s', 'me', 'ma', 'over', 'while', 'its', 'y', 'mustn', 'shant', 'an', 'against', 'youd', 'very', 'between', 'a', 'their', 'him', 'any', 'if', 'under', 'himself', 'doesnt', 'had', 'own', 'hers', 'then', 'you d', 'most', \"won't\", 'aren t', 'for', 'shes', 'she', 'of', 'was', 'so', 'couldnt', 'too', 'at', 'no', 'o', 'i', 'needn', 'isnt', 'werent', 'about', 'been', 'weren t', \"hadn't\", 'havent', 'haven t', 'do', 'will', 'why', 'through', 'wont', 'to', 'hadn', 'shouldve', 'as', 'on', 'how', 'my', 'it', 'shan t', 'what', 'above', 'up', 'didn', 're', 'couldn', 'isn', 'when', 'shouldn t', 'theirs', 'here', \"don't\", 'mustn t', 'isn t', 'who', 'mightnt', 'ain', \"you've\", 'ourselves', 'mightn t', 'these', 'until', 'yours', 't', 'wouldnt', 'you ve', \"she's\", 'such', 's', 'hasn t', 'haven', \"that'll\", 'further', 'be', \"isn't\", 'don t', 'our', \"wouldn't\", 'youll', 'same', 'with', 'wouldn', \"aren't\", 'you', 'your', 'in', 'm', \"needn't\", 'her', 'am', \"you'd\", \"you'll\", 'not', 'arent', 'couldn t', 'and', 'off', 'shouldnt', 'only', 'nor', 'herself', 'she s', \"couldn't\", \"didn't\", \"shouldn't\", 'doesn t', 'mustnt', 'this', 'll', 'his', 'is', \"wasn't\", 'does', 'dont', 'but', \"should've\", 'hasnt', 'thatll', 'wasn t', 'didnt', \"mightn't\", 'wasnt', 'being', 'now', 'you re', 'did', 'during', 'should', \"hasn't\", 'shan', 'other', 'that', \"weren't\", 'by', 'shouldn', 'wouldn t', 'aren', 'again', \"haven't\", 'those', 'that ll', 'which', 'didn t', 'should ve', 'itself', 'before', 'needn t', 'them', 'from'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# if there is a stopword with an apostrophe, add the word without the apostrophe and a space instead of the apostrophe, and the word without the apostrophe\n",
    "more_stop_words = set()\n",
    "for word in stop_words:\n",
    "    if \"'\" in word:\n",
    "        more_stop_words.add(word.replace(\"'\", \" \"))\n",
    "        more_stop_words.add(word.replace(\"'\", \"\"))\n",
    "stop_words = stop_words.union(more_stop_words)\n",
    "\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of negation words, we need to consider the words that are connected to them, so we can preserve the meaning of the sentence by preappending the negation word with a \"not_\" prefix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shouldn t', 'won', \"doesn't\", 'isn t', 'mustn t', 'mightn', 'weren', 'won t', 'mightnt', 'wasn', 'ain', 'mightn t', 'wouldnt', 'hadn t', 'doesn', 'can t', 'hasn t', \"shan't\", 'haven', 'hasn', \"isn't\", 'neednt', \"wouldn't\", \"mustn't\", 'hadnt', 'wouldn', \"aren't\", \"needn't\", 'ma', 'mustn', 'shant', 'not', 'arent', 'doesnt', 'couldn t', 'shouldnt', 'nor', \"couldn't\", \"didn't\", \"shouldn't\", 'doesn t', \"won't\", 'mustnt', 'cant', 'aren t', \"wasn't\", 'couldnt', 'no', \"can't\", 'needn', 'werent', 'isnt', 'hasnt', 'wasn t', 'weren t', 'didnt', \"hadn't\", \"mightn't\", 'wasnt', 'haven t', 'havent', 'wont', 'hadn', \"hasn't\", 'shan', \"weren't\", 'shouldn', 'wouldn t', 'aren', \"haven't\", 'didn t', 'shan t', 'needn t', 'didn', 'couldn', 'isn'}\n"
     ]
    }
   ],
   "source": [
    "# remove negative words from stop_words list\n",
    "\n",
    "neg_stop_words = {'no', 'nor', 'not', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", \"can't\", \"cant\"}\n",
    "\n",
    "more_neg_words = set()\n",
    "# for each negative word with an apostrophe, we will add the word without the apostrophe and a space instead of the apostrophe\n",
    "for word in neg_stop_words:\n",
    "    if \"'\" in word:\n",
    "        more_neg_words.add(word.replace(\"'\", \" \"))\n",
    "        more_neg_words.add(word.replace(\"'\", \"\"))\n",
    "\n",
    "neg_stop_words = neg_stop_words.union(more_neg_words)\n",
    "\n",
    "print(neg_stop_words)\n",
    "\n",
    "# remove from stop_words the negative words\n",
    "stop_words = stop_words - neg_stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "# if a word is in neg_stop_words, we will add a prefix \"not_\" to the word that follows it and remove the negative word\n",
    "def add_not_prefix(text):\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        if words[i] in neg_stop_words:\n",
    "            try:\n",
    "                new_words.append('not_' + words[i+1])\n",
    "            except:\n",
    "                new_words.append('not') # if the negative word is the last word, just add \"not\"\n",
    "            i += 2\n",
    "        else:\n",
    "            new_words.append(words[i])\n",
    "            i += 1\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "df['text'] = df['text'].apply(add_not_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with small words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focusing now on small words, they are not very informative and are in many instances just noise. We can remove them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of small words is 2.65%\n",
      "The 30 most common small words are:\n",
      "im    6201\n",
      "go    1027\n",
      "us     433\n",
      "id     373\n",
      "ok     200\n",
      "oh     119\n",
      "u       72\n",
      "tv      67\n",
      "th      62\n",
      "n       56\n",
      "b       54\n",
      "c       48\n",
      "gt      47\n",
      "e       45\n",
      "w       42\n",
      "pm      36\n",
      "un      35\n",
      "ex      33\n",
      "lt      32\n",
      "st      32\n",
      "p       31\n",
      "mr      27\n",
      "en      25\n",
      "x       25\n",
      "dr      24\n",
      "ya      24\n",
      "co      22\n",
      "f       21\n",
      "ha      20\n",
      "nd      20\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display percentage of words in the dataset that are less than 3 characters \n",
    "\n",
    "words = df['text'].str.split(expand=True).stack()\n",
    "\n",
    "# Get set of unique small words\n",
    "small_words = words[words.str.len() < 3]\n",
    "\n",
    "print(f\"The percentage of small words is {round(small_words[small_words.str.len() < 3].count() / words.count() * 100, 2)}%\") \n",
    "\n",
    "# show the 30 most common small words\n",
    "print(f\"The 30 most common small words are:\\n{small_words.value_counts().head(30)}\") \n",
    "\n",
    "# The only interesting small words are the ones in keep_small_words\n",
    "keep_small_words = {'go', 'ok', 'oh', 'pm', 'am', 'ex'}\n",
    "\n",
    "# remove small words from the dataset and keep the ones in keep_small_words\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if len(word) > 2 or word in keep_small_words]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with bad written words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A thing that we noticed in the dataset is that there are a lot of words that are not written correctly. We can use a spell checker to correct them. But first, one thing that we can do is fix words that are written with more than one letter repeated, like \"soooooo\" or \"amaaaaaazinggggg\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all consecutive same letters\n",
    "def replace_repetitive(text):\n",
    "    pattern = r'(\\w)\\1{2,}'\n",
    "    replaced_text = re.sub(pattern, r'\\1\\1', text)\n",
    "    return replaced_text\n",
    "\n",
    "df['text'] = df['text'].apply(replace_repetitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming and lemmatization\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "def stem(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(stem)\n",
    "df[\"text\"] = df[\"text\"].apply(lemmatize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19132                           feel irrit kinda hate feel\n",
      "51533    rather home feel violent lone not_tri sound in...\n",
      "44351                   suggest wait discus feel le resent\n",
      "51299                                wrong feel royal piss\n",
      "55778    tierd talk like there hope hell care understan...\n",
      "17018    feel frustrat honest like not_get money worth ...\n",
      "45995    tri make chang feel urg happen particularli di...\n",
      "45862                    truli good feel fight ever bother\n",
      "48826                                          feel jealou\n",
      "14602                       feel angri think like elsewher\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Show the first 10 rows\n",
    "print(df[\"text\"].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df to pickle\n",
    "df.to_pickle(\"models/data_processed.pkl\") # NOTE: ONLY SAVING 10% OF THE DATA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
