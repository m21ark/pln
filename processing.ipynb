{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"data.pkl\")\n",
    "\n",
    "# undersample the data\n",
    "df = df.sample(frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some basic cleaning\n",
    "\n",
    "# remove href link\n",
    "df['text'] = df['text'].str.replace(r'href', '')\n",
    "df['text'] = df['text'].str.replace(r'http', '')\n",
    "df['text'] = df['text'].str.replace(r'www', '')\n",
    "\n",
    "df['text'] = df['text'].str.lower() # We need to check if there is a frequent user of capital letters to express emotions\n",
    "df['text'] = df['text'].str.replace(r'[^\\w\\s]', '')\n",
    "df['text'] = df['text'].str.replace(r'\\d+', '')\n",
    "df['text'] = df['text'].str.replace(r'\\n', ' ')\n",
    "df['text'] = df['text'].str.replace(r'\\s+', ' ')\n",
    "df['text'] = df['text'].str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"hasn't\", 'each', 'if', \"don't\", 'should', 'mustn', 'theirs', 'been', 'hasn t', \"wasn't\", 'they', 'in', 'we', 'youd', \"you've\", \"didn't\", 'the', 'aren', 'until', 'didn', 'my', 'during', \"doesn't\", 'youve', 'to', 'is', 'have', 'hasn', 'dont', 'me', 'her', 'further', 'here', 'no', \"shouldn't\", 'just', 'so', 'being', 'yourself', 'off', 'our', 'can', 'ain', 'o', 'hadn', 'themselves', 'most', 'as', 'into', 'y', 'and', 'he', 'youll', 'aren t', 'same', 'that ll', 'hasnt', 'ourselves', 'shouldve', 'are', \"you're\", 'she', 'your', 'on', 'werent', 'having', 'down', 'should ve', 'be', 'when', 't', 'their', 'that', 'doing', \"won't\", 'hadnt', 'mustn t', 'shant', 'its', 'shouldnt', 'hadn t', 'but', 'because', 'by', 'itself', 'at', 'will', 'll', \"wouldn't\", \"aren't\", 'not', 'had', 'shouldn t', 'too', 'below', 'after', 'isn t', 'wasn', 'you', 'his', 'whom', 'an', 'before', 'which', 'while', 'i', 'it s', 'ours', 'such', 'wouldnt', \"should've\", 'you re', 'mightn', 'of', 'didnt', 'herself', 'shes', 'mightn t', 'wouldn', 'all', 'about', 'shan', 'hers', 'haven', \"needn't\", 'what', 'himself', 'these', \"that'll\", 'over', 'how', 'you d', 'wont', \"she's\", 'who', 'or', 'needn t', 'shan t', 'myself', 'then', 'once', 'isn', 'mightnt', 'were', \"mustn't\", 'ma', \"it's\", 'weren t', 'up', \"hadn't\", \"you'd\", 'why', 'with', 'has', 'does', 'for', 'more', 'than', 'don', 're', \"haven't\", 'yourselves', 'wouldn t', 'won t', 'haven t', 'a', 'mustnt', \"shan't\", 'now', 'there', \"you'll\", \"weren't\", 'own', 'couldn t', 'out', 'wasnt', 'through', 'yours', 'was', 'do', 'under', 'few', 'did', 'doesn t', 've', 'd', \"isn't\", 'wasn t', 'neednt', 'between', 'where', \"couldn't\", 'those', 'doesnt', 'other', 'm', 'doesn', 'shouldn', 'him', 'she s', 'won', 'any', 'youre', 'them', 'it', 'only', \"mightn't\", 'both', 'very', 'havent', 'you ll', 'again', 'nor', 'arent', 'isnt', 'against', 'am', 'weren', 'some', 'didn t', 'you ve', 'this', 'from', 'needn', 'above', 's', 'don t', 'thatll', 'couldnt', 'couldn'}\n",
      "{'wouldnt', \"hasn't\", 'mustn', 'mightn', 'hasn t', \"wasn't\", 'didnt', 'mightn t', 'wouldn', 'shan', 'haven', \"needn't\", \"didn't\", 'didn', 'aren', \"doesn't\", 'needn t', 'wont', 'shan t', 'hasn', 'no', 'isn', 'mightnt', \"shouldn't\", \"mustn't\", 'hadn', 'ain', 'ma', \"hadn't\", 'weren t', 'aren t', 'hasnt', \"haven't\", 'wouldn t', 'won t', 'haven t', 'mustnt', \"shan't\", \"weren't\", 'couldn t', 'werent', 'wasnt', 'can t', \"won't\", 'hadnt', 'mustn t', 'doesn t', 'shant', 'shouldnt', 'hadn t', \"isn't\", 'wasn t', 'neednt', \"wouldn't\", \"aren't\", \"couldn't\", 'not', 'doesnt', 'shouldn t', 'doesn', 'shouldn', \"can't\", 'won', 'isn t', 'wasn', \"mightn't\", 'havent', 'cant', 'nor', 'arent', 'isnt', 'weren', 'didn t', 'needn', 'couldnt', 'couldn'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# if there is a stopword with an apostrophe, add the word without the apostrophe and a space instead of the apostrophe, and the word without the apostrophe\n",
    "more_stop_words = set()\n",
    "for word in stop_words:\n",
    "    if \"'\" in word:\n",
    "        more_stop_words.add(word.replace(\"'\", \" \"))\n",
    "        more_stop_words.add(word.replace(\"'\", \"\"))\n",
    "stop_words = stop_words.union(more_stop_words)\n",
    "\n",
    "print(stop_words)\n",
    "\n",
    "\n",
    "# remove negative words from stop_words list\n",
    "\n",
    "neg_stop_words = {'no', 'nor', 'not', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", \"can't\", \"cant\"}\n",
    "\n",
    "more_neg_words = set()\n",
    "# for each negative word with an apostrophe, we will add the word without the apostrophe and a space instead of the apostrophe\n",
    "for word in neg_stop_words:\n",
    "    if \"'\" in word:\n",
    "        more_neg_words.add(word.replace(\"'\", \" \"))\n",
    "        more_neg_words.add(word.replace(\"'\", \"\"))\n",
    "\n",
    "neg_stop_words = neg_stop_words.union(more_neg_words)\n",
    "\n",
    "print(neg_stop_words)\n",
    "\n",
    "stop_words = stop_words - neg_stop_words\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "# if a word is in neg_stop_words, we will add a prefix \"not_\" to the word that follows it and remove the negative word\n",
    "\n",
    "def add_not_prefix(text):\n",
    "    words = text.split()\n",
    "    new_words = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        if words[i] in neg_stop_words:\n",
    "            try:\n",
    "                new_words.append('not_' + words[i+1])\n",
    "            except:\n",
    "                new_words.append('not')\n",
    "            i += 2\n",
    "        else:\n",
    "            new_words.append(words[i])\n",
    "            i += 1\n",
    "    return ' '.join(new_words)\n",
    "\n",
    "df['text'] = df['text'].apply(add_not_prefix)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all words that are less than 3 characters long\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if len(word) > 2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming and lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def lemmatize(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "def stem(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(lemmatize)\n",
    "df[\"text\"] = df[\"text\"].apply(stem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     text emotions\n",
      "27784   face return longwood not_sur feel one hand eag...      joy\n",
      "24235   feel love look sun feel warmth insid feel joy ...      joy\n",
      "66171                   feel terrif terribl weak shall pa      joy\n",
      "5267                                      feel lot better      joy\n",
      "122434                    uncomfort start feel relax zone      joy\n",
      "130128  feel clever say like make brand new mistak eve...      joy\n",
      "118549  feel bit adventur decid tri loreal feria wild ...      joy\n",
      "23863   not_want hurt necessarili feel strong physic a...      joy\n",
      "11176             sit feel like put time machin not_pleas      joy\n",
      "137535  happi bless grate feel amaz level content peac...      joy\n"
     ]
    }
   ],
   "source": [
    "# Show the first 3 with emoitoins = joy\n",
    "print(df[df['emotions'] == 'joy'].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df to pickle\n",
    "df.to_pickle(\"df_v1.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              ngram  count\n",
      "0    feel littl bit    120\n",
      "1     feel like ive    102\n",
      "2    make feel like     73\n",
      "3    feel like need     63\n",
      "4     feel like get     59\n",
      "5   still feel like     54\n",
      "6  feel like punish     48\n",
      "7   feel like peopl     47\n",
      "8   alway feel like     45\n",
      "9  feel like someth     43\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from nltk import ngrams\n",
    "\n",
    "def generate_ngrams(sentence, n):\n",
    "    words = sentence.split()\n",
    "    ngrams = []\n",
    "    for i in range(len(words) - n + 1):\n",
    "        ngram = ' '.join(words[i:i+n])\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate n-grams\n",
    "n = 3\n",
    "ngram_counts = defaultdict(int)\n",
    "for sentence in df['text']:\n",
    "    for ngram in generate_ngrams(sentence, n):\n",
    "        ngram_counts[ngram] += 1\n",
    "\n",
    "ngram_df = pd.DataFrame(sorted(ngram_counts.items(), key=lambda x: x[1])[::-1], columns=['ngram', 'count'])\n",
    "print(ngram_df.head(10))\n",
    "\n",
    "# save ngram_df to pickle\n",
    "ngram_df.to_pickle(\"ngram_v1.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27784</th>\n",
       "      <td>face return longwood not_sur feel one hand eag...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13895</th>\n",
       "      <td>not_help feel ceiliuradh miss opportun acknowl...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11451</th>\n",
       "      <td>feel like retreat back mother place support en...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>almost back track beauti day feel bless</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24235</th>\n",
       "      <td>feel love look sun feel warmth insid feel joy ...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5888</th>\n",
       "      <td>not_feel graciou magnanim feel like curl fetal...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66171</th>\n",
       "      <td>feel terrif terribl weak shall pa</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34584</th>\n",
       "      <td>not_feel total heartless monster may alway tol...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82926</th>\n",
       "      <td>need not_feel worthless</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113501</th>\n",
       "      <td>feel remors</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text emotions\n",
       "27784   face return longwood not_sur feel one hand eag...      joy\n",
       "13895   not_help feel ceiliuradh miss opportun acknowl...  sadness\n",
       "11451   feel like retreat back mother place support en...     love\n",
       "765               almost back track beauti day feel bless     love\n",
       "24235   feel love look sun feel warmth insid feel joy ...      joy\n",
       "5888    not_feel graciou magnanim feel like curl fetal...     love\n",
       "66171                   feel terrif terribl weak shall pa      joy\n",
       "34584   not_feel total heartless monster may alway tol...    anger\n",
       "82926                             need not_feel worthless  sadness\n",
       "113501                                        feel remors  sadness"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41681, 17972)\n",
      "                                                    text emotions\n",
      "27784  face return longwood not_sur feel one hand eag...      joy\n",
      "Index(['back', 'colleg', 'eager', 'face', 'feel', 'freedom', 'friend', 'hand',\n",
      "       'longwood', 'not_sur', 'one', 'return', 'see'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#make a sparce array of the words in the dataset using one hot encoding\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X = vectorizer.fit_transform(df['text']).toarray()\n",
    "print(X.shape)\n",
    "\n",
    "# make a dataframe of the words\n",
    "df_words = pd.DataFrame(X, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "\n",
    "print(df.head(1))\n",
    "\n",
    "#print the columns of df_words that are not 0 for df.head(1)\n",
    "print(df_words.columns[df_words.iloc[0] != 0])\n",
    "\n",
    "\n",
    "# save df_words to pickle                                 WARNING: This file is too big uwu\n",
    "#df_words.to_pickle(\"df_words_v1.pkl\") \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def convert_text_to_embeddings(df, text_column, model_name='all-MiniLM-L6-v2'):\n",
    "\n",
    "    # Load SentenceTransformer model\n",
    "    model = SentenceTransformer(model_name)\n",
    "    \n",
    "    # Get the texts from the DataFrame\n",
    "    texts = df[text_column].tolist()\n",
    "    \n",
    "    # Compute embeddings\n",
    "    embeddings = model.encode(texts)\n",
    "\n",
    "    # insert the embeddings into the DataFrame into a single new column\n",
    "    df['embeddings'] = embeddings.tolist()\n",
    "    \n",
    "        \n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a DataFrame df with a 'text' column\n",
    "embeddings = convert_text_to_embeddings(df, 'text', 'all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.81458732e-02 -5.60483709e-03  8.68722498e-02  4.17182781e-02\n",
      "  1.92652512e-02  3.85539085e-02  6.65507019e-02 -6.02931455e-02\n",
      " -8.92053638e-03 -2.67916750e-02  4.97117154e-02 -4.83248979e-02\n",
      " -3.81650962e-02 -4.73941900e-02  6.84752455e-03 -5.11829481e-02\n",
      " -5.13229333e-03 -3.70376334e-02 -2.56631244e-02  1.07601456e-01\n",
      "  6.60977606e-03  1.83822692e-03 -6.51664510e-02 -3.26131284e-02\n",
      " -2.13531661e-03 -5.26386537e-02 -3.27641070e-02 -3.78051512e-02\n",
      "  6.38306439e-02 -9.13201123e-02  5.57275452e-02  1.32038862e-01\n",
      " -3.58072631e-02  3.95253068e-03 -4.15708199e-02  1.30974427e-01\n",
      " -3.78422923e-02 -5.21705188e-02 -1.13910604e-02  5.93322851e-02\n",
      " -1.29474699e-01  7.89017358e-04  1.51101360e-02  4.78607789e-02\n",
      " -4.01755702e-03 -4.56100218e-02  2.49709561e-02  1.00279246e-02\n",
      "  4.62591685e-02 -7.68105462e-02  1.22880042e-02 -1.31569011e-02\n",
      " -6.09980784e-02 -2.93911854e-03  1.77230891e-02  2.48986315e-02\n",
      " -8.93952232e-03 -8.63799676e-02 -1.81567650e-02  3.69794816e-02\n",
      " -1.28638847e-02 -7.54874665e-03 -2.34167166e-02 -5.28435856e-02\n",
      " -3.42528522e-02  6.48699375e-03 -3.59537452e-02 -6.48617297e-02\n",
      " -7.49015808e-02  4.97485995e-02 -2.70136055e-02 -1.31943244e-02\n",
      " -1.01444669e-01 -5.00367023e-02  5.22297285e-02 -4.11428809e-02\n",
      " -6.33311737e-03 -6.78047314e-02  2.09082440e-02 -2.81051174e-02\n",
      "  4.10326850e-03  6.20895661e-02 -9.48297456e-02  9.34872497e-03\n",
      " -7.60013834e-02 -9.99357328e-02 -4.66842763e-02  7.07266881e-05\n",
      "  1.93048492e-02 -1.78126227e-02 -1.16201453e-02 -3.53102619e-03\n",
      " -8.76861140e-02  2.03438308e-02 -3.44629958e-02 -1.20060975e-02\n",
      " -2.76827924e-02  5.28266467e-02 -1.46107450e-01  9.86545309e-02\n",
      "  9.35519487e-03  5.14010377e-02  2.07626354e-02  4.44751531e-02\n",
      " -3.73178981e-02 -3.55279744e-02  3.98824140e-02 -3.66005115e-03\n",
      " -2.69933604e-03 -1.90696176e-02  7.72415241e-03 -7.79663445e-03\n",
      " -1.09264873e-01  2.99004242e-02  2.74913367e-02  2.58804876e-02\n",
      " -3.50891016e-02 -7.62389302e-02 -1.20835481e-02 -5.75806387e-02\n",
      " -6.81756623e-03  3.68373431e-02 -1.23965200e-02 -3.05942772e-03\n",
      " -3.00876033e-02 -1.74823739e-02  6.50317818e-02  5.08627075e-33\n",
      " -3.00223362e-02  7.56560266e-02 -5.16575463e-02  5.68844425e-03\n",
      "  2.95988061e-02  4.79755551e-03 -7.13091940e-02  1.11642480e-03\n",
      " -5.76927066e-02  9.11968574e-02 -1.04989661e-02  6.42058402e-02\n",
      "  3.62251624e-02 -1.56340636e-02 -5.09567112e-02  5.55586480e-02\n",
      " -9.85077117e-04  9.98775568e-03 -4.54219664e-03 -9.85059142e-03\n",
      " -2.87748873e-03  8.55696350e-02 -3.46851312e-02  4.32232656e-02\n",
      " -5.65147214e-02 -3.29104438e-02 -5.93730190e-04 -6.39282316e-02\n",
      "  7.93504342e-02  3.61731760e-02  4.69707735e-02  8.05690363e-02\n",
      "  4.08069827e-02 -8.18101019e-02  4.60316166e-02  2.45017670e-02\n",
      " -2.58143730e-02 -2.80117039e-02 -5.00866808e-02 -4.57318462e-02\n",
      " -4.45533060e-02  5.27222715e-02 -2.95008440e-02 -3.90206389e-02\n",
      " -2.78977081e-02 -4.14177328e-02  1.14399837e-02  4.70824316e-02\n",
      " -7.16408268e-02 -6.71264529e-02  1.78778209e-02  8.61178562e-02\n",
      " -3.21237445e-02  6.48496673e-02 -6.42340258e-02  3.74754593e-02\n",
      " -6.69314107e-03  1.41944662e-01  2.50518732e-02  2.55201627e-02\n",
      "  8.39473605e-02 -5.07282130e-02 -2.52840947e-02 -3.32306623e-02\n",
      " -7.32506663e-02 -1.65992640e-02 -1.16427010e-02 -2.52611078e-02\n",
      " -1.57577544e-02  1.78794768e-02 -5.05845286e-02  7.13199452e-02\n",
      " -4.55415137e-02 -2.19177008e-02 -4.49498706e-02 -1.05886525e-02\n",
      "  7.04217404e-02  5.59038855e-03  2.91275680e-02 -2.41543464e-02\n",
      " -3.62239145e-02  8.41360260e-03 -6.34513721e-02  2.13326458e-02\n",
      "  5.90268597e-02 -6.24670349e-02 -1.13314176e-02 -1.55974582e-01\n",
      "  1.11291828e-02  7.22105503e-02 -8.97606742e-03 -2.00887583e-03\n",
      " -1.15884719e-02 -3.93459722e-02 -5.86078363e-03 -6.18781181e-33\n",
      "  3.12935188e-02 -6.70291334e-02 -3.19131762e-02 -6.63925037e-02\n",
      "  5.98518513e-02 -9.76724364e-03  3.29924002e-02  1.35195002e-01\n",
      " -3.69723141e-03  4.93569719e-03  4.74169943e-03 -5.92798218e-02\n",
      "  5.81448488e-02 -1.21726030e-02  8.73075575e-02 -1.47624407e-02\n",
      "  7.43388087e-02  1.47820199e-02 -3.03577781e-02  7.26335794e-02\n",
      "  5.05646914e-02  6.82866722e-02  4.89666453e-03  1.21419718e-02\n",
      " -3.86573300e-02  5.18908314e-02  6.20457232e-02 -4.30911370e-02\n",
      " -1.21774487e-01 -8.91534332e-03  2.48668455e-02 -5.32272682e-02\n",
      " -1.39320893e-02  3.54554504e-02 -4.59966585e-02  6.10214137e-02\n",
      " -8.39911029e-02  5.59777161e-03 -8.05937275e-02  6.44340217e-02\n",
      "  1.85868219e-02 -4.70206812e-02  3.10300291e-02  7.36406371e-02\n",
      "  2.92606652e-02 -4.14077900e-02 -4.10713740e-02 -4.21553478e-02\n",
      "  5.39061725e-02  8.52662027e-02  4.66186926e-02  5.85928932e-02\n",
      " -1.26602473e-02  8.00350588e-03  4.14929353e-03 -1.00683130e-01\n",
      " -5.63475909e-03 -7.16265291e-02  3.48001234e-02 -3.02141290e-02\n",
      " -2.20515467e-02 -9.51904580e-02 -4.67432365e-02  2.98536513e-02\n",
      "  1.55020982e-01 -3.77352629e-03 -4.54049148e-02 -1.35178426e-02\n",
      "  3.61664109e-02 -8.98882840e-03  1.78632326e-02 -2.30940357e-02\n",
      " -5.97646385e-02 -2.01343391e-02  7.54020810e-02  6.34957626e-02\n",
      " -6.68667257e-02 -2.58200280e-02 -2.29291245e-02  6.10176139e-02\n",
      " -3.90407033e-02 -6.61254395e-03  2.20471155e-02  6.87017962e-02\n",
      "  2.53475066e-02 -6.12206478e-03  6.27385750e-02  1.11379415e-01\n",
      "  6.15352672e-03 -3.70557569e-02  2.05753855e-02  2.88770869e-02\n",
      "  5.31433858e-02  1.43824157e-03  3.60364094e-02 -2.63112092e-08\n",
      " -7.63348415e-02 -2.50946585e-04  7.49189109e-02 -5.75545095e-02\n",
      "  2.44453922e-02  7.26707950e-02  3.64632159e-02 -1.52566819e-03\n",
      " -1.23124845e-01  7.04083741e-02 -1.78829245e-02  8.28725696e-02\n",
      "  3.37586440e-02  2.58203433e-03 -2.85208635e-02  5.82731999e-02\n",
      " -2.64558885e-02 -2.48562843e-02 -3.01385205e-02 -2.03806423e-02\n",
      " -2.55286712e-02 -3.75572704e-02  9.51295421e-02  3.36618386e-02\n",
      " -5.96786104e-02 -1.61804184e-02 -2.89394334e-02  2.54025366e-02\n",
      " -1.39343012e-02 -2.55522095e-02  5.03805019e-02  4.28575985e-02\n",
      " -4.15497869e-02 -5.37983933e-03 -3.21922228e-02 -4.50698455e-04\n",
      "  2.90356800e-02  3.02339904e-02  7.17377663e-02  1.51824772e-01\n",
      "  1.15119964e-02  1.18157588e-01  1.17694996e-02  5.59966005e-02\n",
      " -3.03022820e-03  5.50188534e-02  8.39347988e-02 -1.53850066e-02\n",
      "  1.96422227e-02 -6.76230341e-03  4.10410110e-03 -9.67211276e-02\n",
      " -4.45153676e-02  3.97259481e-02 -1.81741714e-02  1.99992340e-02\n",
      " -7.01973587e-02  9.02463198e-02  3.34410369e-02 -2.58532166e-02\n",
      "  6.81085698e-03 -2.43364908e-02 -7.70312771e-02  2.95205694e-02]\n"
     ]
    }
   ],
   "source": [
    "# print a line of the embeddings\n",
    "print(embeddings[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle the embeddings\n",
    "# this wittle file is twooooo bwiig uwu\n",
    "#df.to_pickle(\"df_embeddings_v1.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO List\n",
    "\n",
    "- Stopwords\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "- N-grams\n",
    "- TF-IDF\n",
    "- Word Embeddings\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
