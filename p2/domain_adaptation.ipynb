{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Adaptation\n",
    "\n",
    "In this notebook, we are going to perform domain adaptation on the distilbert model, using our dataset. Instead of just performing the regular fine-tuning, we are going to use the Masked Language Model (MLM) objective to train the model. \n",
    "\n",
    "The idea is to train the model in this way, and then see if it can perform better than the regular fine-tuning approach.\n",
    "\n",
    "[Reference](https://towardsdatascience.com/fine-tuning-for-domain-adaptation-in-nlp-c47def356fd6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-18 20:50:33.383656: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-18 20:50:33.642463: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-18 20:50:33.642644: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-18 20:50:33.678317: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-18 20:50:33.748706: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-18 20:50:35.869628: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "from datasets import load_metric\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TextClassificationPipeline\n",
    "from transformers import pipeline\n",
    "from transformers import BertForMaskedLM, DistilBertForMaskedLM\n",
    "from transformers import BertTokenizer, DistilBertTokenizer\n",
    "from transformers import DataCollatorForLanguageModeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2084, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>emotions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98790</th>\n",
       "      <td>i wanted them to feel now i feel as though i a...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18398</th>\n",
       "      <td>i found myself feeling very sympathetic toward...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139349</th>\n",
       "      <td>ive been feeling pretty good today and tonight...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13416</th>\n",
       "      <td>i can use to cover my ass when i feel inadequate</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22297</th>\n",
       "      <td>i feel like ive pissed myself again</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text emotions\n",
       "98790   i wanted them to feel now i feel as though i a...  sadness\n",
       "18398   i found myself feeling very sympathetic toward...     love\n",
       "139349  ive been feeling pretty good today and tonight...      joy\n",
       "13416    i can use to cover my ass when i feel inadequate  sadness\n",
       "22297                 i feel like ive pissed myself again    anger"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"data/data_original.pkl\")\n",
    "\n",
    "down_sample_percentage = 0.5\n",
    "\n",
    "df = df.sample(frac=down_sample_percentage/100, random_state=1)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'emotions', '__index_level_0__'],\n",
      "    num_rows: 1667\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'emotions', '__index_level_0__'],\n",
      "    num_rows: 417\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34151298455433fa4e2ada8ae48cad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1667 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a13328923094cf48c764df0fa20a746",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/417 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, return_special_tokens_mask=True)\n",
    "\n",
    "column_names = train_dataset.column_names\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True,\n",
    "    num_proc=4, # multiprocessing.cpu_count() I am not sure if this is a good idea, since it might use all cores?\n",
    "    remove_columns=column_names\n",
    ")\n",
    "\n",
    "test_dataset = test_dataset.map(\n",
    "    tokenize_function, \n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sparks/.local/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30fc08ab2f1f4dae938b4901e6480daf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/209 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219cf0bb22e54a5ea7e108527bcd4fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.7863354682922363, 'eval_runtime': 215.3701, 'eval_samples_per_second': 1.936, 'eval_steps_per_second': 0.246, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['vocab_projector.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 3337.4522, 'train_samples_per_second': 0.499, 'train_steps_per_second': 0.063, 'train_loss': 2.976363624682267, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, \n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./domain-model\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./domain-model/distilbert-emotions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./domain-model/distilbert-emotions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./domain-model/distilbert-emotions/tokenizer_config.json',\n",
       " './domain-model/distilbert-emotions/special_tokens_map.json',\n",
       " './domain-model/distilbert-emotions/vocab.txt',\n",
       " './domain-model/distilbert-emotions/added_tokens.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"./domain-model/distilbert-emotions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2872af0f75764cccbcbada3160888b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.8271596431732178,\n",
       " 'eval_runtime': 218.0222,\n",
       " 'eval_samples_per_second': 1.913,\n",
       " 'eval_steps_per_second': 0.243,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
